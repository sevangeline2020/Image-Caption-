{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "intership project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7a0907bbf2ee43aca7d484c5d9881e69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e9ecd16fe194039a87613177bada0bf",
              "IPY_MODEL_eed8e6837cff403398e7ba77be055505",
              "IPY_MODEL_6f876d823b5b4e83a06f7bc8e614abe1"
            ],
            "layout": "IPY_MODEL_4cce9f80e7ab4459adeaacecfbfce7e1"
          }
        },
        "4e9ecd16fe194039a87613177bada0bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96fbbbd1230749f09426a6ace3d4f4ee",
            "placeholder": "​",
            "style": "IPY_MODEL_9ddcc3ff6bab4852b6c950a118bd8fc8",
            "value": ""
          }
        },
        "eed8e6837cff403398e7ba77be055505": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55381c42cd44458abb1da72236c0f868",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd9448aabc3149089d3e0f9561319e2a",
            "value": 0
          }
        },
        "6f876d823b5b4e83a06f7bc8e614abe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b8ec246e3824973a64cb3c7239e7001",
            "placeholder": "​",
            "style": "IPY_MODEL_741d4650d4e948cab30c189792f3bf49",
            "value": " 0/? [00:00&lt;?, ?it/s]"
          }
        },
        "4cce9f80e7ab4459adeaacecfbfce7e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96fbbbd1230749f09426a6ace3d4f4ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ddcc3ff6bab4852b6c950a118bd8fc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55381c42cd44458abb1da72236c0f868": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "dd9448aabc3149089d3e0f9561319e2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b8ec246e3824973a64cb3c7239e7001": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "741d4650d4e948cab30c189792f3bf49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IMAGE CAPTION GENERATOR"
      ],
      "metadata": {
        "id": "gKQx-9cE9gbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install keras.utils to_categorical"
      ],
      "metadata": {
        "id": "F2Ci8TTp9qYf",
        "outputId": "63d6d3b8-b8ac-4cd2-afcd-780ff2ee0691",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package keras.utils\n",
            "E: Couldn't find any package by glob 'keras.utils'\n",
            "E: Couldn't find any package by regex 'keras.utils'\n",
            "E: Unable to locate package to_categorical\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from pickle import dump, load\n",
        "import numpy as np\n",
        "\n",
        "from keras.applications.xception import Xception, preprocess_input\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers.merge import add\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
        "\n",
        "# small library for seeing the progress of loops.\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "tqdm().pandas()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "7a0907bbf2ee43aca7d484c5d9881e69",
            "4e9ecd16fe194039a87613177bada0bf",
            "eed8e6837cff403398e7ba77be055505",
            "6f876d823b5b4e83a06f7bc8e614abe1",
            "4cce9f80e7ab4459adeaacecfbfce7e1",
            "96fbbbd1230749f09426a6ace3d4f4ee",
            "9ddcc3ff6bab4852b6c950a118bd8fc8",
            "55381c42cd44458abb1da72236c0f868",
            "dd9448aabc3149089d3e0f9561319e2a",
            "3b8ec246e3824973a64cb3c7239e7001",
            "741d4650d4e948cab30c189792f3bf49"
          ]
        },
        "id": "EIe54bBjd_1F",
        "outputId": "0713e298-0708-4502-a908-d51f7028cf29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a0907bbf2ee43aca7d484c5d9881e69"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Text Data"
      ],
      "metadata": {
        "id": "6hRvN-lydb7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a text file into memory\n",
        "def load_doc(filename):\n",
        "    # Opening the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n"
      ],
      "metadata": {
        "id": "ZoOA3gOPWQT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize Images with captions"
      ],
      "metadata": {
        "id": "y3bTdC8ydtYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get all imgs with their captions\n",
        "def all_img_captions(filename):\n",
        "    file = load_doc(filename)\n",
        "    captions = file.split('\\n')\n",
        "    descriptions ={}\n",
        "    for caption in captions[:-1]:\n",
        "        img, caption = caption.split('\\t')\n",
        "        if img[:-2] not in descriptions:\n",
        "            descriptions[img[:-2]] = [ caption ]\n",
        "        else:\n",
        "            descriptions[img[:-2]].append(caption)\n",
        "    return descriptions\n",
        "\n",
        "#Data cleaning- lower casing, removing puntuations and words containing numbers\n",
        "def cleaning_text(captions):\n",
        "    table = str.maketrans('','',string.punctuation)\n",
        "    for img,caps in captions.items():\n",
        "        for i,img_caption in enumerate(caps):\n",
        "\n",
        "            img_caption.replace(\"-\",\" \")\n",
        "            desc = img_caption.split()\n",
        "\n",
        "            #converts to lowercase\n",
        "            desc = [word.lower() for word in desc]\n",
        "            #remove punctuation from each token\n",
        "            desc = [word.translate(table) for word in desc]\n",
        "            #remove hanging 's and a \n",
        "            desc = [word for word in desc if(len(word)>1)]\n",
        "            #remove tokens with numbers in them\n",
        "            desc = [word for word in desc if(word.isalpha())]\n",
        "            #convert back to string\n",
        "\n",
        "            img_caption = ' '.join(desc)\n",
        "            captions[img][i]= img_caption\n",
        "    return captions\n",
        "\n",
        "def text_vocabulary(descriptions):\n",
        "    # build vocabulary of all unique words\n",
        "    vocab = set()\n",
        "\n",
        "    for key in descriptions.keys():\n",
        "        [vocab.update(d.split()) for d in descriptions[key]]\n",
        "\n",
        "    return vocab\n",
        "\n",
        "#All descriptions in one file \n",
        "def save_descriptions(descriptions, filename):\n",
        "    lines = list()\n",
        "    for key, desc_list in descriptions.items():\n",
        "        for desc in desc_list:\n",
        "            lines.append(key + '\\t' + desc )\n",
        "    data = \"\\n\".join(lines)\n",
        "    file = open(filename,\"w\")\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ySYgzfyGbLJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selecting Model"
      ],
      "metadata": {
        "id": "ZquKrIkZd6yU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set these path according to project folder in you system\n",
        "dataset_image = \"../input/flickr8kimagescaptions/flickr8k/images\"\n",
        "dataset_text = \"../input/flickr8kimagescaptions/flickr8k/captions.txt\"\n",
        "\n",
        "#we prepare our text data\n",
        "filename = dataset_text + \"/\" + \"Flickr8k.token.txt\"\n",
        "#loading the file that contains all data\n",
        "#mapping them into descriptions dictionary img to 5 captions\n",
        "descriptions = all_img_captions(filename)\n",
        "print(\"Length of descriptions =\" ,len(descriptions))\n",
        "\n",
        "#cleaning the descriptions\n",
        "clean_descriptions = cleaning_text(descriptions)\n",
        "\n",
        "#building vocabulary \n",
        "vocabulary = text_vocabulary(clean_descriptions)\n",
        "print(\"Length of vocabulary = \", len(vocabulary))\n",
        "\n",
        "#saving each description to file \n",
        "save_descriptions(clean_descriptions, \"descriptions.txt\")"
      ],
      "metadata": {
        "id": "QZ7sSZFPd5E0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "d8ac52ad-ee51-4e39-ab94-706dae41425b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-fa7393701428>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#loading the file that contains all data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#mapping them into descriptions dictionary img to 5 captions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdescriptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_img_captions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Length of descriptions =\"\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'all_img_captions' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(directory):\n",
        "        model = Xception( include_top=False, pooling='avg' )\n",
        "        features = {}\n",
        "        for img in tqdm(os.listdir(directory)):\n",
        "            filename = directory + \"/\" + img\n",
        "            image = Image.open(filename)\n",
        "            image = image.resize((299,299))\n",
        "            image = np.expand_dims(image, axis=0)\n",
        "            #image = preprocess_input(image)\n",
        "            image = image/127.5\n",
        "            image = image - 1.0\n",
        "\n",
        "            feature = model.predict(image)\n",
        "            features[img] = feature\n",
        "        return features\n",
        "\n",
        "#2048 feature vector\n",
        "features = extract_features(dataset_images)\n",
        "dump(features, open(\"features.p\",\"wb\"))"
      ],
      "metadata": {
        "id": "acDlu2b2SJYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = load(open(\"features.p\",\"rb\"))"
      ],
      "metadata": {
        "id": "ceJJnAu2SVoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#dataset training a model\n",
        "\n",
        "In our Flickr_8k_test folder, we have Flickr_8k.trainImages.txt file that contains a list of 6000 image names that we will use for training.\n",
        "\n",
        "For loading the training dataset, we need more functions:\n",
        "\n",
        "load_photos( filename ) – This will load the text file in a string and will return the list of image names.\n",
        "\n",
        "load_clean_descriptions( filename, photos ) – This function will create a dictionary that contains captions for each photo from the list of photos. We also append the <start> and <end> identifier for each caption. We need this so that our LSTM model can identify the starting and ending of the caption.\n",
        "\n",
        "\n",
        "load_features(photos) – This function will give us the dictionary for image names and their feature vector which we have previously extracted from the Xception model."
      ],
      "metadata": {
        "id": "gUxtDW-_ZdYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the data \n",
        "def load_photos(filename):\n",
        "    file = load_doc(filename)\n",
        "    photos = file.split(\"\\n\")[:-1]\n",
        "    return photos\n",
        "\n",
        "\n",
        "def load_clean_descriptions(filename, photos): \n",
        "    #loading clean_descriptions\n",
        "    file = load_doc(filename)\n",
        "    descriptions = {}\n",
        "    for line in file.split(\"\\n\"):\n",
        "\n",
        "        words = line.split()\n",
        "        if len(words)<1 :\n",
        "            continue\n",
        "\n",
        "        image, image_caption = words[0], words[1:]\n",
        "\n",
        "        if image in photos:\n",
        "            if image not in descriptions:\n",
        "                descriptions[image] = []\n",
        "            desc = '<start> ' + \" \".join(image_caption) + ' <end>'\n",
        "            descriptions[image].append(desc)\n",
        "\n",
        "    return descriptions\n",
        "\n",
        "\n",
        "def load_features(photos):\n",
        "    #loading all features\n",
        "    all_features = load(open(\"features.p\",\"rb\"))\n",
        "    #selecting only needed features\n",
        "    features = {k:all_features[k] for k in photos}\n",
        "    return features\n",
        "\n",
        "\n",
        "filename = dataset_text + \"/\" + \"Flickr_8k.trainImages.txt\"\n",
        "\n",
        "#train = loading_data(filename)\n",
        "train_imgs = load_photos(filename)\n",
        "train_descriptions = load_clean_descriptions(\"descriptions.txt\", train_imgs)\n",
        "train_features = load_features(train_imgs)"
      ],
      "metadata": {
        "id": "ePrHNI3uShPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting dictionary to clean list of descriptions\n",
        "def dict_to_list(descriptions):\n",
        "    all_desc = []\n",
        "    for key in descriptions.keys():\n",
        "        [all_desc.append(d) for d in descriptions[key]]\n",
        "    return all_desc\n",
        "\n",
        "#creating tokenizer class \n",
        "#this will vectorise text corpus\n",
        "#each integer will represent token in dictionary\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "def create_tokenizer(descriptions):\n",
        "    desc_list = dict_to_list(descriptions)\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(desc_list)\n",
        "    return tokenizer\n",
        "\n",
        "# give each word an index, and store that into tokenizer.p pickle file\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "dump(tokenizer, open('tokenizer.p', 'wb'))\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "EFUFjSr6Slvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate maximum length of descriptions\n",
        "def max_length(descriptions):\n",
        "    desc_list = dict_to_list(descriptions)\n",
        "    return max(len(d.split()) for d in desc_list)\n",
        "    \n",
        "max_length = max_length(descriptions)\n",
        "max_length"
      ],
      "metadata": {
        "id": "IC7clwYuSpun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create input-output sequence pairs from the image description.\n",
        "\n",
        "#data generator, used by model.fit_generator()\n",
        "def data_generator(descriptions, features, tokenizer, max_length):\n",
        "    while 1:\n",
        "        for key, description_list in descriptions.items():\n",
        "            #retrieve photo features\n",
        "            feature = features[key][0]\n",
        "            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, feature)\n",
        "            yield [[input_image, input_sequence], output_word]\n",
        "\n",
        "def create_sequences(tokenizer, max_length, desc_list, feature):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    # walk through each description for the image\n",
        "    for desc in desc_list:\n",
        "        # encode the sequence\n",
        "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "        # split one sequence into multiple X,y pairs\n",
        "        for i in range(1, len(seq)):\n",
        "            # split into input and output pair\n",
        "            in_seq, out_seq = seq[:i], seq[i]\n",
        "            # pad input sequence\n",
        "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "            # encode output sequence\n",
        "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "            # store\n",
        "            X1.append(feature)\n",
        "            X2.append(in_seq)\n",
        "            y.append(out_seq)\n",
        "    return np.array(X1), np.array(X2), np.array(y)\n",
        "\n",
        "#You can check the shape of the input and output for your model\n",
        "[a,b],c = next(data_generator(train_descriptions, features, tokenizer, max_length))\n",
        "a.shape, b.shape, c.shape\n",
        "#((47, 2048), (47, 32), (47, 7577))"
      ],
      "metadata": {
        "id": "RycrTVkdSsnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import plot_model\n",
        "\n",
        "# define the captioning model\n",
        "def define_model(vocab_size, max_length):\n",
        "\n",
        "    # features from the CNN model squeezed from 2048 to 256 nodes\n",
        "    inputs1 = Input(shape=(2048,))\n",
        "    fe1 = Dropout(0.5)(inputs1)\n",
        "    fe2 = Dense(256, activation='relu')(fe1)\n",
        "\n",
        "    # LSTM sequence model\n",
        "    inputs2 = Input(shape=(max_length,))\n",
        "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "    se2 = Dropout(0.5)(se1)\n",
        "    se3 = LSTM(256)(se2)\n",
        "\n",
        "    # Merging both models\n",
        "    decoder1 = add([fe2, se3])\n",
        "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "    # tie it together [image, seq] [word]\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "    # summarize model\n",
        "    print(model.summary())\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "g4qg0vcWSxA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Training the model\n",
        "\n",
        "To train the model, we will be using the 6000 training images by generating the input and output sequences in batches and fitting them to the model using model.fit_generator() method. We also save the model to our models folder. This will take some time depending on your system capability"
      ],
      "metadata": {
        "id": "fQnDfLKNXO7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train our model\n",
        "print('Dataset: ', len(train_imgs))\n",
        "print('Descriptions: train=', len(train_descriptions))\n",
        "print('Photos: train=', len(train_features))\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Description Length: ', max_length)\n",
        "\n",
        "model = define_model(vocab_size, max_length)\n",
        "epochs = 10\n",
        "steps = len(train_descriptions)\n",
        "# making a directory models to save our models\n",
        "os.mkdir(\"models\")\n",
        "for i in range(epochs):\n",
        "    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
        "    model.fit_generator(generator, epochs=1, steps_per_epoch= steps, verbose=1)\n",
        "    model.save(\"models/model_\" + str(i) + \".h5\")"
      ],
      "metadata": {
        "id": "jqZ4aM0nS1uO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Testing the model\n",
        "\n",
        "The model has been trained, now, we will make a separate file testing_caption_generator.py which will load the model and generate predictions. The predictions contain the max length of index values so we will use the same tokenizer.p pickle file to get the words from their index values."
      ],
      "metadata": {
        "id": "ObXAMg0pXZf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "\n",
        "\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument('-i', '--image', required=True, help=\"Image Path\")\n",
        "args = vars(ap.parse_args())\n",
        "img_path = args['image']\n",
        "\n",
        "def extract_features(filename, model):\n",
        "        try:\n",
        "            image = Image.open(filename)\n",
        "\n",
        "        except:\n",
        "            print(\"ERROR: Couldn't open image! Make sure the image path and extension is correct\")\n",
        "        image = image.resize((299,299))\n",
        "        image = np.array(image)\n",
        "        # for images that has 4 channels, we convert them into 3 channels\n",
        "        if image.shape[2] == 4: \n",
        "            image = image[..., :3]\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "        image = image/127.5\n",
        "        image = image - 1.0\n",
        "        feature = model.predict(image)\n",
        "        return feature\n",
        "\n",
        "def word_for_id(integer, tokenizer):\n",
        "for word, index in tokenizer.word_index.items():\n",
        "     if index == integer:\n",
        "         return word\n",
        "return None\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ckRxKzWqVF0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing\n",
        "\n",
        "Computers don’t understand English words, for computers, we will have to represent them with numbers. So, we will map each word of the vocabulary with a unique index value. Keras library provides us with the tokenizer function that we will use to create tokens from our vocabulary and save them to a “tokenizer.p” pickle file."
      ],
      "metadata": {
        "id": "Me6YcDQxejVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "    in_text = 'start'\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        pred = model.predict([photo,sequence], verbose=0)\n",
        "        pred = np.argmax(pred)\n",
        "        word = word_for_id(pred, tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == 'end':\n",
        "            break\n",
        "    return in_text\n"
      ],
      "metadata": {
        "id": "no3AfpbYXnAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#path = 'Flicker8k_Dataset/111537222_07e56d5a30.jpg'\n",
        "max_length = 32\n",
        "tokenizer = load(open(\"tokenizer.p\",\"rb\"))\n",
        "model = load_model('models/model_9.h5')\n",
        "xception_model = Xception(include_top=False, pooling=\"avg\")\n",
        "\n",
        "photo = extract_features(img_path, xception_model)\n",
        "img = Image.open(img_path)\n",
        "\n",
        "description = generate_desc(model, tokenizer, photo, max_length)\n",
        "print(\"\\n\\n\")\n",
        "print(description)\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "RV1fjotNXiiG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}